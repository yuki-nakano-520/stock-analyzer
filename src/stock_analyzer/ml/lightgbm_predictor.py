"""LightGBMã‚’ä½¿ã£ãŸé«˜åº¦ãªæ ªä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’ç”¨ï¼‰"""

import logging
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import joblib
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, train_test_split

warnings.filterwarnings("ignore", category=UserWarning)


def _get_logger() -> Any:
    """ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—"""
    try:
        from ..utils.logging_config import get_logger

        return get_logger(__name__, module="lightgbm_predictor")
    except ImportError:
        import logging

        return logging.getLogger(__name__)


logger: Any = _get_logger()


class LightGBMStockPredictor:
    """
    LightGBMã‚’ä½¿ã£ãŸé«˜åº¦ãªæ ªä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«

    ç‰¹å¾´:
    - é«˜ç²¾åº¦ãªã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°
    - è‡ªå‹•çš„ãªç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    - æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹å …ç‰¢ãªè©•ä¾¡
    - è¤‡æ•°äºˆæ¸¬æœŸé–“ã¸ã®å¯¾å¿œ
    """

    def __init__(self, model_name: str = "stock_predictor"):
        """
        LightGBMæ ªä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–

        Args:
            model_name (str): ãƒ¢ãƒ‡ãƒ«å
        """
        logger.info(f"LightGBMäºˆæ¸¬ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {model_name}")

        self.model_name = model_name
        self.models: Dict[str, lgb.LGBMRegressor] = {}
        self.feature_names: List[str] = []
        self.is_fitted = False
        self.feature_importance: Dict[str, float] = {}

        # LightGBMã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.lgb_params = {
            "objective": "regression",
            "metric": "rmse",
            "boosting_type": "gbdt",
            "num_leaves": 31,
            "learning_rate": 0.05,
            "feature_fraction": 0.9,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbose": -1,
            "random_state": 42,
        }

    def prepare_training_data(
        self, features: pd.DataFrame, targets: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        Args:
            features (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            targets (pd.DataFrame): ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿

        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿(ç‰¹å¾´é‡, ç›®çš„å¤‰æ•°)
        """
        logger.debug("LightGBMè¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹")

        # æ•°å€¤å‹ã®ç‰¹å¾´é‡ã®ã¿ã‚’é¸æŠ
        numeric_features = features.select_dtypes(include=[np.number])

        # ç„¡é™å€¤ã¨NaNã‚’å‡¦ç†
        numeric_features = numeric_features.replace([np.inf, -np.inf], np.nan)
        targets = targets.replace([np.inf, -np.inf], np.nan)

        # æ¬ æå€¤ãŒå¤šã™ãã‚‹åˆ—ã‚’å‰Šé™¤ï¼ˆ50%ä»¥ä¸Šï¼‰
        nan_ratio = numeric_features.isnull().sum() / len(numeric_features)
        high_nan_columns = nan_ratio[nan_ratio > 0.5].index
        if len(high_nan_columns) > 0:
            logger.warning(f"æ¬ æå€¤ãŒå¤šã„ç‰¹å¾´é‡ã‚’å‰Šé™¤: {list(high_nan_columns)}")
            numeric_features = numeric_features.drop(columns=high_nan_columns)

        # æ®‹ã£ãŸæ¬ æå€¤ã‚’å‰æ–¹åŸ‹ã‚
        numeric_features = numeric_features.ffill().bfill().fillna(0)
        targets = targets.ffill().bfill().fillna(0)

        # ç‰¹å¾´é‡åã‚’ä¿å­˜
        self.feature_names = list(numeric_features.columns)

        logger.debug(
            f"LightGBMè¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: ç‰¹å¾´é‡{numeric_features.shape}, ç›®çš„å¤‰æ•°{targets.shape}"
        )

        return numeric_features, targets

    def train_model(
        self,
        features: pd.DataFrame,
        targets: pd.DataFrame,
        target_columns: List[str] | None = None,
        test_size: float = 0.2,
        n_splits: int = 5,
    ) -> Dict[str, Dict[str, float]]:
        """
        è¤‡æ•°ã®ç›®çš„å¤‰æ•°ã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

        Args:
            features (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            targets (pd.DataFrame): ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿
            target_columns (List[str]): äºˆæ¸¬å¯¾è±¡ã®åˆ—åãƒªã‚¹ãƒˆ
            test_size (float): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ
            n_splits (int): äº¤å·®æ¤œè¨¼ã®åˆ†å‰²æ•°

        Returns:
            Dict[str, Dict[str, float]]: å„ç›®çš„å¤‰æ•°ã®è©•ä¾¡çµæœ
        """
        if target_columns is None:
            target_columns = ["return_5d", "return_30d"]
        logger.info(f"LightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {target_columns}")

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X, y_all = self.prepare_training_data(features, targets)

        results = {}

        for target_col in target_columns:
            if target_col not in y_all.columns:
                logger.warning(f"ç›®çš„å¤‰æ•° '{target_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            logger.info(f"ç›®çš„å¤‰æ•° '{target_col}' ã®è¨“ç·´é–‹å§‹")

            y = y_all[target_col]

            # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            valid_indices = ~(
                X.isnull().any(axis=1) | y.isnull() | np.isinf(y) | np.isnan(y)
            )
            X_clean = X[valid_indices].copy()  # Ensure DataFrame type
            y_clean = y[valid_indices].copy()  # Ensure Series type

            # Type assertions to help pyright
            if not isinstance(X_clean, pd.DataFrame):
                raise TypeError("X_clean should be DataFrame")
            if not isinstance(y_clean, pd.Series):
                raise TypeError("y_clean should be Series")

            logger.debug(
                f"æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_clean)}/{len(X)} ({len(X_clean) / len(X) * 100:.1f}%)"
            )

            # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X_clean, y_clean, test_size=test_size, random_state=42, shuffle=False
            )

            # LightGBMãƒ¢ãƒ‡ãƒ«ä½œæˆ
            model = lgb.LGBMRegressor(
                n_estimators=1000, early_stopping_rounds=50, **self.lgb_params
            )

            # è¨“ç·´
            model.fit(
                X_train,
                y_train,
                eval_set=[(X_test, y_test)],
                callbacks=[lgb.log_evaluation(0)],  # ãƒ­ã‚°å‡ºåŠ›ã‚’æŠ‘åˆ¶
            )

            # äºˆæ¸¬ã¨è©•ä¾¡
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)

            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            train_metrics = {
                "rmse": np.sqrt(mean_squared_error(y_train, y_pred_train)),
                "mae": mean_absolute_error(y_train, y_pred_train),
                "r2": r2_score(y_train, y_pred_train),
            }

            test_metrics = {
                "rmse": np.sqrt(mean_squared_error(y_test, y_pred_test)),
                "mae": mean_absolute_error(y_test, y_pred_test),
                "r2": r2_score(y_test, y_pred_test),
            }

            # æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼
            cv_scores = self._cross_validate_timeseries(
                model, X_clean, y_clean, n_splits
            )

            results[target_col] = {
                "train_metrics": train_metrics,
                "test_metrics": test_metrics,
                "cv_metrics": cv_scores,
                "feature_importance": dict(
                    zip(self.feature_names, model.feature_importances_, strict=False)
                ),
            }

            # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            self.models[target_col] = model

            logger.info(
                f"'{target_col}' è¨“ç·´å®Œäº† - ãƒ†ã‚¹ãƒˆRÂ²: {test_metrics['r2']:.3f}, RMSE: {test_metrics['rmse']:.3f}"
            )

        self.is_fitted = True

        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’çµ±åˆ
        self._aggregate_feature_importance()

        logger.info(f"å…¨ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(results)}å€‹ã®ãƒ¢ãƒ‡ãƒ«")

        return results

    def _cross_validate_timeseries(
        self, model: lgb.LGBMRegressor, X: pd.DataFrame, y: pd.Series, n_splits: int
    ) -> Dict[str, float]:
        """
        æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«é©ã—ãŸäº¤å·®æ¤œè¨¼ã‚’å®Ÿè¡Œ

        Args:
            model: LightGBMãƒ¢ãƒ‡ãƒ«
            X: ç‰¹å¾´é‡
            y: ç›®çš„å¤‰æ•°
            n_splits: åˆ†å‰²æ•°

        Returns:
            Dict[str, float]: äº¤å·®æ¤œè¨¼çµæœ
        """
        logger.debug(f"æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼é–‹å§‹: {n_splits}åˆ†å‰²")

        tscv = TimeSeriesSplit(n_splits=n_splits)
        cv_scores = {"rmse": [], "mae": [], "r2": []}

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
            X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]
            y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]

            # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            fold_model = lgb.LGBMRegressor(**self.lgb_params, n_estimators=500)
            fold_model.fit(
                X_fold_train, y_fold_train, callbacks=[lgb.log_evaluation(0)]
            )

            y_fold_pred = fold_model.predict(X_fold_val)

            # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
            cv_scores["rmse"].append(
                np.sqrt(mean_squared_error(y_fold_val, y_fold_pred))
            )
            cv_scores["mae"].append(mean_absolute_error(y_fold_val, y_fold_pred))
            cv_scores["r2"].append(r2_score(y_fold_val, y_fold_pred))

            logger.debug(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold + 1}/{n_splits} å®Œäº†")

        # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
        cv_results = {
            "rmse_mean": float(np.mean(cv_scores["rmse"])),
            "rmse_std": float(np.std(cv_scores["rmse"])),
            "mae_mean": float(np.mean(cv_scores["mae"])),
            "mae_std": float(np.std(cv_scores["mae"])),
            "r2_mean": float(np.mean(cv_scores["r2"])),
            "r2_std": float(np.std(cv_scores["r2"])),
        }

        logger.debug(
            f"äº¤å·®æ¤œè¨¼å®Œäº† - RÂ²: {cv_results['r2_mean']:.3f} (Â±{cv_results['r2_std']:.3f})"
        )

        return cv_results

    def _aggregate_feature_importance(self) -> None:
        """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’çµ±åˆ"""
        if not self.models:
            return

        # å…¨ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¹³å‡
        importance_sum = {}
        for model_name, model in self.models.items():
            logger.debug(f"ç‰¹å¾´é‡é‡è¦åº¦ã‚’é›†è¨ˆä¸­: {model_name}")
            for feature, importance in zip(
                self.feature_names, model.feature_importances_, strict=False
            ):
                if feature not in importance_sum:
                    importance_sum[feature] = []
                importance_sum[feature].append(importance)

        # å¹³å‡ã‚’è¨ˆç®—
        self.feature_importance = {
            feature: np.mean(importances)
            for feature, importances in importance_sum.items()
        }

        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        self.feature_importance = dict(
            sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)[
                :20
            ]  # ä¸Šä½20å€‹
        )

    def predict(
        self,
        features: pd.DataFrame,
        target_columns: Optional[List[str]] = None,
    ) -> Dict[str, np.ndarray]:
        """
        æ ªä¾¡äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Args:
            features (pd.DataFrame): äºˆæ¸¬ç”¨ç‰¹å¾´é‡
            target_columns (Optional[List[str]]): äºˆæ¸¬ã™ã‚‹ç›®çš„å¤‰æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ã¦ï¼‰

        Returns:
            Dict[str, np.ndarray]: å„ç›®çš„å¤‰æ•°ã®äºˆæ¸¬çµæœ
        """
        if not self.is_fitted:
            raise ValueError(
                "ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«train_model()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )

        if target_columns is None:
            target_columns = list(self.models.keys())

        logger.debug(f"äºˆæ¸¬å®Ÿè¡Œ: {target_columns}")

        # ç‰¹å¾´é‡ã‚’æº–å‚™
        X = features[self.feature_names].fillna(0)

        predictions = {}
        for target_col in target_columns:
            if target_col not in self.models:
                logger.warning(f"ãƒ¢ãƒ‡ãƒ« '{target_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            pred = self.models[target_col].predict(X)
            predictions[target_col] = pred
            # Safe length calculation for different prediction result types
            if hasattr(pred, "shape") and pred.shape:
                pred_count = pred.shape[0]
            elif hasattr(pred, "__len__") and not hasattr(
                pred, "toarray"
            ):  # Avoid spmatrix
                try:
                    pred_count = len(pred)  # type: ignore[arg-type]
                except TypeError:
                    pred_count = (
                        getattr(pred, "shape", [1])[0] if hasattr(pred, "shape") else 1
                    )
            else:
                pred_count = (
                    getattr(pred, "shape", [1])[0] if hasattr(pred, "shape") else 1
                )
            logger.debug(f"'{target_col}' äºˆæ¸¬å®Œäº†: {pred_count}ä»¶")

        return predictions

    def get_feature_importance(self, top_n: int = 20) -> Dict[str, float]:
        """
        ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—

        Args:
            top_n (int): å–å¾—ã™ã‚‹ä¸Šä½Nå€‹

        Returns:
            Dict[str, float]: ç‰¹å¾´é‡é‡è¦åº¦
        """
        if not self.feature_importance:
            logger.warning("ç‰¹å¾´é‡é‡è¦åº¦ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return {}

        return dict(list(self.feature_importance.items())[:top_n])

    def save_models(self, save_dir: str = "models") -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜

        Args:
            save_dir (str): ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        if not self.is_fitted:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)

        # å„ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        for target_col, model in self.models.items():
            model_file = save_path / f"{self.model_name}_{target_col}.joblib"
            joblib.dump(model, model_file)
            logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_file}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        metadata = {
            "model_name": self.model_name,
            "feature_names": self.feature_names,
            "feature_importance": self.feature_importance,
            "target_columns": list(self.models.keys()),
        }

        metadata_file = save_path / f"{self.model_name}_metadata.joblib"
        joblib.dump(metadata, metadata_file)
        logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {metadata_file}")

    def load_models(self, save_dir: str = "models") -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            save_dir (str): èª­ã¿è¾¼ã¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        save_path = Path(save_dir)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        metadata_file = save_path / f"{self.model_name}_metadata.joblib"
        if not metadata_file.exists():
            raise FileNotFoundError(
                f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {metadata_file}"
            )

        metadata = joblib.load(metadata_file)
        self.feature_names = metadata["feature_names"]
        self.feature_importance = metadata["feature_importance"]

        # å„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        self.models = {}
        for target_col in metadata["target_columns"]:
            model_file = save_path / f"{self.model_name}_{target_col}.joblib"
            if model_file.exists():
                self.models[target_col] = joblib.load(model_file)
                logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_file}")
            else:
                logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_file}")

        self.is_fitted = len(self.models) > 0
        logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(self.models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«")


# ä½¿ç”¨ä¾‹ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å®Ÿè¡Œã—ãŸæ™‚ã®ã¿å‹•ä½œï¼‰
if __name__ == "__main__":
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    import logging

    logging.basicConfig(
        level=logging.INFO, format="[%(levelname)s] %(name)s: %(message)s"
    )

    print("=== LightGBMæ ªä¾¡äºˆæ¸¬ãƒ†ã‚¹ãƒˆ ===")
    try:
        from ..analysis.features import clean_features, create_all_features
        from ..data.fetchers import get_stock_data

        # 2å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§ååˆ†ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºä¿
        data = get_stock_data("AAPL", "2y")
        print(
            f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {data.index[0].strftime('%Y-%m-%d')} ï½ {data.index[-1].strftime('%Y-%m-%d')}"
        )

        # ç‰¹å¾´é‡ä½œæˆ
        features, targets = create_all_features(data)
        features, targets = clean_features(features, targets)

        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
        print(f"ç‰¹å¾´é‡: {features.shape}")
        print(f"ç›®çš„å¤‰æ•°: {targets.shape}")

        # LightGBMãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»è¨“ç·´
        print("\nğŸ¤– LightGBMãƒ¢ãƒ‡ãƒ«è¨“ç·´:")
        predictor = LightGBMStockPredictor("AAPL_predictor")

        # è¤‡æ•°æœŸé–“ã‚’äºˆæ¸¬
        results = predictor.train_model(
            features, targets, target_columns=["return_5d", "return_30d"], n_splits=3
        )

        # çµæœè¡¨ç¤º
        for target_col, result in results.items():
            test_metrics = result["test_metrics"]
            cv_metrics = result["cv_metrics"]

            # Type assertion to ensure these are dictionaries
            if not isinstance(test_metrics, dict):
                raise TypeError("test_metrics should be dict")
            if not isinstance(cv_metrics, dict):
                raise TypeError("cv_metrics should be dict")

            print(f"\nğŸ“ˆ {target_col} äºˆæ¸¬çµæœ:")
            print(f"ãƒ†ã‚¹ãƒˆ RÂ²: {test_metrics['r2']:.3f}")
            print(f"ãƒ†ã‚¹ãƒˆ RMSE: {test_metrics['rmse']:.3f}")
            print(
                f"äº¤å·®æ¤œè¨¼ RÂ²: {cv_metrics['r2_mean']:.3f} (Â±{cv_metrics['r2_std']:.3f})"
            )

        # ç‰¹å¾´é‡é‡è¦åº¦è¡¨ç¤º
        print("\nğŸ” é‡è¦ç‰¹å¾´é‡ (ä¸Šä½10å€‹):")
        importance = predictor.get_feature_importance(10)
        for feature, score in importance.items():
            print(f"{feature}: {score:.3f}")

        # äºˆæ¸¬å®Ÿè¡Œ
        print("\nğŸ¯ æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬:")
        latest_features = features.tail(1)
        predictions = predictor.predict(latest_features)

        for target_col, pred in predictions.items():
            days = target_col.split("_")[1][:-1]  # "5d" -> "5"
            print(f"{days}æ—¥å¾Œãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬: {pred[0]:.2f}%")

        print("\nâœ… LightGBMãƒ†ã‚¹ãƒˆå®Œäº†")

    except ImportError:
        print("å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("uv run python -m stock_analyzer.ml.lightgbm_predictor")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
