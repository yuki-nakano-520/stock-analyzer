"""scikit-learnã‚’ä½¿ã£ãŸåŸºæœ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’ç”¨ï¼‰"""

import logging
from typing import Any, Dict, List, Tuple

import joblib
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
)
from sklearn.model_selection import TimeSeriesSplit, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC, SVR


def _get_logger() -> Any:
    """ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—"""
    try:
        from ..utils.logging_config import get_logger

        return get_logger(__name__, module="ml_models")
    except ImportError:
        import logging

        return logging.getLogger(__name__)


logger: Any = _get_logger()


class StockPricePredictor:
    """
    æ ªä¾¡äºˆæ¸¬ã®ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’ç”¨ï¼‰

    å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç‰¹å¾´:
    - LinearRegression: æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã€è§£é‡ˆã—ã‚„ã™ã„
    - RandomForest: éç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆã‚‹ã€éå­¦ç¿’ã«å¼·ã„
    - SVR: è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã€å°ãƒ‡ãƒ¼ã‚¿ã§ã‚‚æœ‰åŠ¹
    """

    def __init__(self, model_type: str = "random_forest"):
        """
        æ ªä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–

        Args:
            model_type (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« ('linear', 'random_forest', 'svr')
        """
        logger.info(f"æ ªä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {model_type}")

        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names: List[str] = []
        self.is_fitted = False

        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if model_type == "linear":
            self.model = LinearRegression()
            logger.debug("ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        elif model_type == "random_forest":
            self.model = RandomForestRegressor(
                n_estimators=100, max_depth=10, random_state=42, n_jobs=-1
            )
            logger.debug("ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆå›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        elif model_type == "svr":
            self.model = SVR(kernel="rbf", C=100, gamma="scale")
            logger.debug("ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")

    def prepare_data(
        self,
        features: pd.DataFrame,
        targets: pd.DataFrame,
        target_column: str = "return_5d",
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ©Ÿæ¢°å­¦ç¿’ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        Args:
            features (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            targets (pd.DataFrame): ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿
            target_column (str): äºˆæ¸¬å¯¾è±¡ã®åˆ—å

        Returns:
            Tuple[np.ndarray, np.ndarray]: (X, y) å½¢å¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        """
        logger.debug(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹: target={target_column}")

        # ç›®çš„å¤‰æ•°ã®é¸æŠ
        if target_column not in targets.columns:
            logger.error(f"ç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_column}")
            raise ValueError(f"ç›®çš„å¤‰æ•° '{target_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        y = targets[target_column].values

        # ç‰¹å¾´é‡ã®æº–å‚™ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
        numeric_features = features.select_dtypes(include=[np.number])
        X = numeric_features.values
        self.feature_names = list(numeric_features.columns)

        # NaNã®å‡¦ç†
        valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))
        X = X[valid_indices]
        y = y[valid_indices]

        logger.debug(f"ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: X.shape={X.shape}, y.shape={y.shape}")
        logger.info(
            f"æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}/{len(features)} ({len(X)/len(features)*100:.1f}%)"
        )

        return X, y

    def train(
        self, X: np.ndarray, y: np.ndarray, test_size: float = 0.2
    ) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

        Args:
            X (np.ndarray): ç‰¹å¾´é‡
            y (np.ndarray): ç›®çš„å¤‰æ•°
            test_size (float): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ

        Returns:
            Dict[str, Any]: è¨“ç·´çµæœ
        """
        logger.info(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {self.model_type}")

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )

        logger.debug(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: train={len(X_train)}, test={len(X_test)}")

        # ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        logger.debug("ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†")

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.model.fit(X_train_scaled, y_train)
        self.is_fitted = True

        logger.debug("ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred_train = self.model.predict(X_train_scaled)
        y_pred_test = self.model.predict(X_test_scaled)

        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        results = {
            "model_type": self.model_type,
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "train_mse": mean_squared_error(y_train, y_pred_train),
            "test_mse": mean_squared_error(y_test, y_pred_test),
            "train_mae": mean_absolute_error(y_train, y_pred_train),
            "test_mae": mean_absolute_error(y_test, y_pred_test),
            "train_r2": r2_score(y_train, y_pred_train),
            "test_r2": r2_score(y_test, y_pred_test),
        }

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆRandom Forestã®ã¿ï¼‰
        if hasattr(self.model, "feature_importances_"):
            importances = self.model.feature_importances_
            feature_importance = dict(
                zip(self.feature_names, importances, strict=False)
            )
            results["feature_importance"] = dict(
                sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[
                    :10
                ]
            )  # ä¸Šä½10å€‹ã®ã¿

        logger.info(
            f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº† - ãƒ†ã‚¹ãƒˆRÂ²: {results['test_r2']:.3f}, ãƒ†ã‚¹ãƒˆMAE: {results['test_mae']:.3f}"
        )

        return results

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        äºˆæ¸¬ã‚’å®Ÿè¡Œ

        Args:
            X (np.ndarray): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿

        Returns:
            np.ndarray: äºˆæ¸¬çµæœ
        """
        if not self.is_fitted:
            raise ValueError(
                "ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã«train()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
            )

        logger.debug(f"äºˆæ¸¬å®Ÿè¡Œ: ã‚µãƒ³ãƒ—ãƒ«æ•°={len(X)}")

        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)

        logger.debug("äºˆæ¸¬å®Œäº†")

        return predictions

    def cross_validate(
        self, X: np.ndarray, y: np.ndarray, cv: int = 5
    ) -> Dict[str, Any]:
        """
        äº¤å·®æ¤œè¨¼ã‚’å®Ÿè¡Œ

        Args:
            X (np.ndarray): ç‰¹å¾´é‡
            y (np.ndarray): ç›®çš„å¤‰æ•°
            cv (int): åˆ†å‰²æ•°

        Returns:
            Dict[str, Any]: äº¤å·®æ¤œè¨¼çµæœ
        """
        logger.info(f"äº¤å·®æ¤œè¨¼é–‹å§‹: {cv}åˆ†å‰²")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_scaled = self.scaler.fit_transform(X)

        # äº¤å·®æ¤œè¨¼ï¼ˆæ™‚ç³»åˆ—ã‚’è€ƒæ…®ï¼‰
        tscv = TimeSeriesSplit(n_splits=cv)
        scores = cross_val_score(self.model, X_scaled, y, cv=tscv, scoring="r2")

        results = {
            "cv_scores": scores.tolist(),
            "mean_score": scores.mean(),
            "std_score": scores.std(),
            "cv_folds": cv,
        }

        logger.info(
            f"äº¤å·®æ¤œè¨¼å®Œäº† - å¹³å‡RÂ²: {results['mean_score']:.3f} (Â±{results['std_score']:.3f})"
        )

        return results

    def save_model(self, filepath: str) -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜

        Args:
            filepath (str): ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        if not self.is_fitted:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

        logger.debug(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {filepath}")

        model_data = {
            "model": self.model,
            "scaler": self.scaler,
            "model_type": self.model_type,
            "feature_names": self.feature_names,
        }

        joblib.dump(model_data, filepath)
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {filepath}")

    def load_model(self, filepath: str) -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿

        Args:
            filepath (str): ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        logger.debug(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {filepath}")

        model_data = joblib.load(filepath)

        self.model = model_data["model"]
        self.scaler = model_data["scaler"]
        self.model_type = model_data["model_type"]
        self.feature_names = model_data["feature_names"]
        self.is_fitted = True

        logger.info(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {filepath}")


class StockDirectionClassifier:
    """
    æ ªä¾¡ã®æ–¹å‘æ€§ï¼ˆä¸Šæ˜‡/ä¸‹é™ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹åˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆå­¦ç¿’ç”¨ï¼‰
    """

    def __init__(self, model_type: str = "random_forest"):
        """
        æ ªä¾¡æ–¹å‘æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–

        Args:
            model_type (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ« ('logistic', 'random_forest', 'svc')
        """
        logger.info(f"æ ªä¾¡æ–¹å‘æ€§äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {model_type}")

        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names: List[str] = []
        self.is_fitted = False

        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        if model_type == "logistic":
            self.model = LogisticRegression(random_state=42, max_iter=1000)
            logger.debug("ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        elif model_type == "random_forest":
            self.model = RandomForestClassifier(
                n_estimators=100, max_depth=10, random_state=42, n_jobs=-1
            )
            logger.debug("ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        elif model_type == "svc":
            self.model = SVC(kernel="rbf", C=100, gamma="scale", random_state=42)
            logger.debug("ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")

    def prepare_data(
        self,
        features: pd.DataFrame,
        targets: pd.DataFrame,
        target_column: str = "direction_5d",
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        åˆ†é¡ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™

        Args:
            features (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            targets (pd.DataFrame): ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿
            target_column (str): äºˆæ¸¬å¯¾è±¡ã®åˆ—å

        Returns:
            Tuple[np.ndarray, np.ndarray]: (X, y) å½¢å¼ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
        """
        logger.debug(f"åˆ†é¡ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹: target={target_column}")

        # ç›®çš„å¤‰æ•°ã®é¸æŠ
        if target_column not in targets.columns:
            logger.error(f"ç›®çš„å¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {target_column}")
            raise ValueError(f"ç›®çš„å¤‰æ•° '{target_column}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        y = targets[target_column].values

        # ç‰¹å¾´é‡ã®æº–å‚™ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
        numeric_features = features.select_dtypes(include=[np.number])
        X = numeric_features.values
        self.feature_names = list(numeric_features.columns)

        # NaNã®å‡¦ç†
        valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))
        X = X[valid_indices]
        y = y[valid_indices]

        logger.debug(f"åˆ†é¡ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: X.shape={X.shape}, y.shape={y.shape}")
        logger.info(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: ä¸Šæ˜‡={sum(y)}ä»¶, ä¸‹é™={len(y)-sum(y)}ä»¶")

        return X, y

    def train(
        self, X: np.ndarray, y: np.ndarray, test_size: float = 0.2
    ) -> Dict[str, Any]:
        """
        åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´

        Args:
            X (np.ndarray): ç‰¹å¾´é‡
            y (np.ndarray): ç›®çš„å¤‰æ•°ï¼ˆ0 or 1ï¼‰
            test_size (float): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²åˆ

        Returns:
            Dict[str, Any]: è¨“ç·´çµæœ
        """
        logger.info(f"åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {self.model_type}")

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        logger.debug(f"ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: train={len(X_train)}, test={len(X_test)}")

        # ç‰¹å¾´é‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        logger.debug("ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†")

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.model.fit(X_train_scaled, y_train)
        self.is_fitted = True

        logger.debug("åˆ†é¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")

        # äºˆæ¸¬ã¨è©•ä¾¡
        y_pred_train = self.model.predict(X_train_scaled)
        y_pred_test = self.model.predict(X_test_scaled)

        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        results = {
            "model_type": self.model_type,
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "train_accuracy": accuracy_score(y_train, y_pred_train),
            "test_accuracy": accuracy_score(y_test, y_pred_test),
            "classification_report": classification_report(
                y_test, y_pred_test, output_dict=True
            ),
            "confusion_matrix": confusion_matrix(y_test, y_pred_test).tolist(),
        }

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆRandom Forestã®ã¿ï¼‰
        if hasattr(self.model, "feature_importances_"):
            importances = self.model.feature_importances_
            feature_importance = dict(
                zip(self.feature_names, importances, strict=False)
            )
            results["feature_importance"] = dict(
                sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[
                    :10
                ]
            )  # ä¸Šä½10å€‹ã®ã¿

        logger.info(f"åˆ†é¡ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº† - ãƒ†ã‚¹ãƒˆç²¾åº¦: {results['test_accuracy']:.3f}")

        return results


def compare_models(
    features: pd.DataFrame,
    targets: pd.DataFrame,
    target_column: str = "return_5d",
    task_type: str = "regression",
) -> Dict[str, Any]:
    """
    è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒè©•ä¾¡

    Args:
        features (pd.DataFrame): ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        targets (pd.DataFrame): ç›®çš„å¤‰æ•°ãƒ‡ãƒ¼ã‚¿
        target_column (str): äºˆæ¸¬å¯¾è±¡ã®åˆ—å
        task_type (str): ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ— ('regression' or 'classification')

    Returns:
        Dict[str, Any]: å„ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒçµæœ
    """
    logger.info(f"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒé–‹å§‹: {task_type}, target={target_column}")

    results = {"task_type": task_type, "target_column": target_column, "models": {}}

    if task_type == "regression":
        model_types = ["linear", "random_forest", "svr"]

        for model_type in model_types:
            logger.debug(f"ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­: {model_type}")

            try:
                # ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨è¨“ç·´
                predictor = StockPricePredictor(model_type)
                X, y = predictor.prepare_data(features, targets, target_column)

                # äº¤å·®æ¤œè¨¼
                cv_results = predictor.cross_validate(X, y, cv=5)

                # é€šå¸¸ã®è¨“ç·´ãƒ»è©•ä¾¡
                train_results = predictor.train(X, y)

                # çµæœçµ±åˆ
                results["models"][model_type] = {
                    **train_results,
                    "cross_validation": cv_results,
                }

                logger.debug(f"{model_type} è©•ä¾¡å®Œäº†")

            except Exception as e:
                logger.error(f"{model_type} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results["models"][model_type] = {"error": str(e)}

    elif task_type == "classification":
        model_types = ["logistic", "random_forest", "svc"]

        for model_type in model_types:
            logger.debug(f"åˆ†é¡ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­: {model_type}")

            try:
                # ãƒ¢ãƒ‡ãƒ«ä½œæˆã¨è¨“ç·´
                classifier = StockDirectionClassifier(model_type)
                X, y = classifier.prepare_data(features, targets, target_column)

                # é€šå¸¸ã®è¨“ç·´ãƒ»è©•ä¾¡
                train_results = classifier.train(X, y)

                results["models"][model_type] = train_results

                logger.debug(f"{model_type} åˆ†é¡è©•ä¾¡å®Œäº†")

            except Exception as e:
                logger.error(f"{model_type} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                results["models"][model_type] = {"error": str(e)}

    logger.info(f"ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Œäº†: {len(results['models'])}ãƒ¢ãƒ‡ãƒ«")

    return results


# ä½¿ç”¨ä¾‹ï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å®Ÿè¡Œã—ãŸæ™‚ã®ã¿å‹•ä½œï¼‰
if __name__ == "__main__":
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    import logging

    logging.basicConfig(
        level=logging.INFO, format="[%(levelname)s] %(name)s: %(message)s"
    )

    print("=== scikit-learnæ©Ÿæ¢°å­¦ç¿’ãƒ†ã‚¹ãƒˆ ===")
    try:
        from ..analysis.features import clean_features, create_all_features
        from ..data.fetchers import get_stock_data

        # 1å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã§ååˆ†ãªå±¥æ­´ã‚’ç¢ºä¿
        data = get_stock_data("AAPL", "1y")
        print(
            f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {data.index[0].strftime('%Y-%m-%d')} ï½ {data.index[-1].strftime('%Y-%m-%d')}"
        )

        # ç‰¹å¾´é‡ä½œæˆ
        features, targets = create_all_features(data)
        features, targets = clean_features(features, targets)

        print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
        print(f"ç‰¹å¾´é‡: {features.shape}")
        print(f"ç›®çš„å¤‰æ•°: {targets.shape}")

        # å›å¸°ã‚¿ã‚¹ã‚¯ã®ãƒ†ã‚¹ãƒˆ
        print("\nğŸ¤– å›å¸°ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
        regression_results = compare_models(
            features, targets, "return_5d", "regression"
        )

        for model_name, result in regression_results["models"].items():
            if "error" not in result:
                print(
                    f"{model_name}: RÂ²={result['test_r2']:.3f}, MAE={result['test_mae']:.3f}"
                )
                if "cross_validation" in result:
                    cv = result["cross_validation"]
                    print(
                        f"  äº¤å·®æ¤œè¨¼RÂ²: {cv['mean_score']:.3f} (Â±{cv['std_score']:.3f})"
                    )
            else:
                print(f"{model_name}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")

        # åˆ†é¡ã‚¿ã‚¹ã‚¯ã®ãƒ†ã‚¹ãƒˆ
        print("\nğŸ¯ åˆ†é¡ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ:")
        classification_results = compare_models(
            features, targets, "direction_5d", "classification"
        )

        for model_name, result in classification_results["models"].items():
            if "error" not in result:
                print(f"{model_name}: ç²¾åº¦={result['test_accuracy']:.3f}")
            else:
                print(f"{model_name}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")

        print("\nâœ… æ©Ÿæ¢°å­¦ç¿’ãƒ†ã‚¹ãƒˆå®Œäº†")

    except ImportError:
        print("å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("uv run python -m stock_analyzer.ml.basic_models")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
